"""
BEREL Test v2 - Expanded Corpus + Hybrid Approach

This test:
1. Uses a larger sample corpus (~50 texts instead of 5)
2. Tests a HYBRID approach: Vector search â†’ Claude confirmation
3. Shows real-world performance expectations
"""

from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import json

print("Loading BEREL model...")
tokenizer = AutoTokenizer.from_pretrained("dicta-il/BEREL")
model = AutoModel.from_pretrained("dicta-il/BEREL")
print("âœ“ Model loaded\n")

def embed_text(text):
    """Create BEREL embedding for text"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    # Use [CLS] token embedding
    embedding = outputs.last_hidden_state[:, 0, :].numpy()
    return embedding

# EXPANDED CORPUS - More texts for better testing
SAMPLE_CORPUS = [
    # Kesubos
    {"ref": "Kesubos 2a", "text": "× ×¢×¨×” ×©× ×ª×¤×ª×ª×” ××‘×™×” ×‘×›×ª×•×‘×ª×” ×•×‘×•×©×ª×” ×•×¤×’×ž×” × ×¢×¨×” ×”×ž××•×¨×¡×” ××‘×™×” ×‘×§× ×¡×”"},
    {"ref": "Kesubos 2b", "text": "×”×ž×•×¦×™× ×©× ×¨×¢ ×¢×œ ×”×§×˜× ×” ×¤×˜×•×¨ ×©××™× ×• × ×•×ª×Ÿ ××œ× ×œ×ž×•×¦×™× ×©× ×¨×¢ ×¢×œ ×”×‘×•×’×¨×ª"},
    {"ref": "Kesubos 9a", "text": "×¡×¤×§ ×¡×¤×™×§× ×”×•× ×©×ž× ×œ× ×‘×¢×œ ×•×©×ž× ×‘×¢×œ ×ž×¢×•×‘×¨×ª ×—×‘×™×¨×• ×”×™×"},
    {"ref": "Kesubos 12a", "text": "×©×•×™×™×” ×× ×¤×©×™×” ×—×ª×™×›×” ×“××™×¡×•×¨× ×•××ž×¨ ×¨×‘× ×‘×¨×™×” ×“×¨×‘× ×ž×“×œ×ž×"},
    {"ref": "Kesubos 12b", "text": "×’×ž×¨× ××ž×¨ ×¨×‘×” ×‘×¨ ×‘×¨ ×—× ×” ××ž×¨ ×¨×‘×™ ×™×•×—× ×Ÿ ×©×•×™×™×” ×× ×¤×©×™×” ×—×ª×™×›×” ×“××™×¡×•×¨×"},
    {"ref": "Kesubos 22a", "text": "×¢×“ ××—×“ × ××ž×Ÿ ×‘××™×¡×•×¨×™×Ÿ ×ž× × ×œ×Ÿ ××ž×¨ ×¨×‘×™ ××¡×™ ××ž×¨ ×¨×‘×™ ×™×•×—× ×Ÿ"},
    {"ref": "Kesubos 22b", "text": "×—×–×§×” ××™×Ÿ ××“× ×¢×•×©×” ×‘×¢×™×œ×ª×• ×‘×¢×™×œ×ª ×–× ×•×ª ×‘×¢×œ ×›×“×™ × ×™×©×•××™×Ÿ ×‘×¢×œ"},
    
    # Bava Metzia - ×‘×¨×™ ×•×©×ž×
    {"ref": "Bava Metzia 5b", "text": "×ž×¡×¤×™×§× ×“×“×™× × ×‘×¨×™ ×•×©×ž× ×‘×¨×™ ×¢×“×™×£ ×“×œ× ××¤×§×™× ×Ÿ ×ž×™× ×™×” ×ž×ž×•× × ×“××™× ×™×©"},
    {"ref": "Bava Metzia 6a", "text": "×”×•×” ××‘×™×™ ×•×”×•×” ×¨×‘× ×—×“ ××ž×¨ ×‘×¨×™ ×•×©×ž× ×‘×¨×™ ×¢×“×™×£ ×•×—×“ ××ž×¨ ×‘×¨×™ ×•×©×ž× ×ž×ž×•× × ×ž×•×¦×™×"},
    {"ref": "Bava Metzia 97b", "text": "×‘×¨×™ ×©×œ×™ ×•×©×ž× ×©×œ×š ×‘×¨×™ ×¢×“×™×£ ××ž×¨ ×¨×‘× ×”×œ×›×” ×‘×¨×™ ×•×©×ž× ×‘×¨×™ ×¢×“×™×£"},
    
    # Kiddushin - ×©×•×•×™× ×× ×¤×©×™×”
    {"ref": "Kiddushin 2b", "text": "×”××™×© ×ž×§×“×© ×‘××™×©×” ×•××™×Ÿ ×”××©×” ×ž×§×“×©×ª ×‘××™×©"},
    {"ref": "Kiddushin 3a", "text": "×‘×›×¡×£ ×‘×©×˜×¨ ×•×‘×‘×™××” ×‘×›×¡×£ ×›×™×¦×“ × ×ª×Ÿ ×œ×” ×›×¡×£ ××• ×©×•×•×” ×›×¡×£"},
    {"ref": "Kiddushin 19a", "text": "×ž××™ ×“×›×ª×™×‘ ×•×”×™×” ×œ×š ×œ××•×ª ×¢×œ ×™×“×š ×•×œ×–×›×¨×•×Ÿ ×‘×™×Ÿ ×¢×™× ×™×š"},
    {"ref": "Kiddushin 65b", "text": "×”××•×ž×¨ ×œ××©×” ×”×ª×§×“×©×™ ×œ×™ ×‘×›×•×¡ ×–×” ×©×•×•×™× ×× ×¤×©×™×” ×—×ª×™×›×” ×“××™×¡×•×¨×"},
    
    # Chullin - ×—×–×§×ª ×¨×‘ ×”×•× ×
    {"ref": "Chullin 10a", "text": "××ž×¨ ×¨×‘ ×”×•× × ×—×–×§×” ××™×Ÿ ××“× ×ž×•×¦×™× ×“×‘×¨ ×ž×ª×—×ª ×™×“×• ×œ×›×ª×—×™×œ×” ×œ××™×¡×•×¨"},
    {"ref": "Chullin 10b", "text": "×—×–×§×” ×–×• ×“×¨×‘ ×”×•× × ×—×–×§×” ×“×ž××™ ×ž×¢×©×” ×©×”×™×” ×›×š ×”×™×”"},
    {"ref": "Chullin 11a", "text": "××ž×¨ ×¨×‘× ×—×–×§×ª ×¨×‘ ×”×•× × ×œ×ž×” ×œ×™ ×‘×¨×™ ×•×©×ž× ×‘×¨×™ ×¢×“×™×£"},
    {"ref": "Chullin 12a", "text": "×•×¨×‘ ×”×•× × ×¡×‘×¨ ×—×–×§×” ×’×“×•×œ×” ×™×© ×œ× ×• ×‘×–×” ×©××™×Ÿ ××“×"},
    
    # Gittin - ×’×˜
    {"ref": "Gittin 2a", "text": "×”×ž×‘×™× ×’×˜ ×ž×ž×“×™× ×ª ×”×™× ×¦×¨×™×š ×©×™××ž×¨ ×‘×¤× ×™ × ×›×ª×‘ ×•×‘×¤× ×™ × ×—×ª×"},
    {"ref": "Gittin 20a", "text": "×’×˜ ×¤×©×•×˜ ×¢×“×™×• ×ž×ª×•×›×• ×’×˜ ×ž×§×•×©×¨ ×¢×“×™×• ×ž××—×•×¨×™×•"},
    {"ref": "Gittin 85b", "text": "×”×©×•×œ×— ×’×˜ ×œ××©×ª×• ×•×¤×’×¢ ×‘×• ×‘×“×¨×š ×ž×‘×˜×œ×• ×‘×¤× ×™×” ×•×‘×¤× ×™ ×©× ×™×"},
    
    # Bava Basra - ×—×–×§×”
    {"ref": "Bava Basra 28a", "text": "×—×–×§×” ×©×œ×•×© ×©× ×™× ×©× ×” ×¨××©×•× ×” ×©× ×™×™×” ×•×©×œ×™×©×™×ª"},
    {"ref": "Bava Basra 41a", "text": "×—×–×§×” ×‘×ž×§×•× ×©×™×© ×¢×“×™× ××¤×™×œ×• ×™×•× ××—×“ ×—×–×§×”"},
    {"ref": "Bava Basra 41b", "text": "×—×–×§×” ××™×Ÿ ××“× ×¤×•×¨×¢ ×ª×•×š ×–×ž× ×•"},
    
    # Pesachim - ×¡×¤×§ ×¡×¤×™×§×
    {"ref": "Pesachim 9a", "text": "×¡×¤×§ ×¡×¤×™×§× ×œ×”×§×œ ×©×ž× ×œ× × ×›× ×¡ ×•×©×ž× × ×›× ×¡ ×›×‘×¨ ×‘×™×¢×¨×•"},
    {"ref": "Pesachim 9b", "text": "×¡×¤×§ ×—×ž×¥ ×‘×¨×©×•×ª ×”×¨×‘×™× ×ž×•×ª×¨ ×‘×¨×©×•×ª ×”×™×—×™×“ ××¡×•×¨"},
    
    # More varied texts
    {"ref": "Shabbos 19a", "text": "×¡×¤×§ ×—×©×›×” ×¡×¤×§ ××™× ×” ×—×©×›×” ×¡×¤×§ ×¡×¤×™×§× ×œ×”×§×œ"},
    {"ref": "Yevamos 31a", "text": "×¢×“ ××—×“ × ××ž×Ÿ ×‘××™×¡×•×¨×™×Ÿ ×“××ž×¨ ×§×¨× ×¢×œ ×¤×™ ×©× ×™× ×¢×“×™×"},
    {"ref": "Sanhedrin 3b", "text": "×‘×¨×™ ×•×©×ž× ×‘×¨×™ ×¢×“×™×£ ×“×œ× ××¤×§×™× ×Ÿ ×ž×™× ×™×” ×ž×ž×•× ×"},
]

print(f"Creating embeddings for {len(SAMPLE_CORPUS)} texts...")
corpus_embeddings = []
for item in SAMPLE_CORPUS:
    emb = embed_text(item["text"])
    corpus_embeddings.append(emb)
corpus_embeddings = np.vstack(corpus_embeddings)
print("âœ“ Embeddings created\n")

# TEST QUERIES
TEST_QUERIES = [
    "chezkas rav huna",           # Should find Chullin 10a strongly
    "shaviya anafshe chaticha deisura",  # Should find Kesubos 12b strongly
    "bari vishma",                # Should find Bava Metzia texts
    "sfek sfeka",                 # Should find Pesachim/Kesubos
    "eid echad neeman beissurin", # Should find Kesubos 22a / Yevamos
]

print("="*70)
print("TESTING WITH EXPANDED CORPUS (30 texts)")
print("="*70)

for query in TEST_QUERIES:
    print(f"\nðŸ” Query: '{query}'")
    print("-"*70)
    
    # Get query embedding
    query_emb = embed_text(query)
    
    # Calculate similarities
    similarities = cosine_similarity(query_emb, corpus_embeddings)[0]
    
    # Get top 3 matches
    top_indices = np.argsort(similarities)[-3:][::-1]
    
    for i, idx in enumerate(top_indices, 1):
        score = similarities[idx]
        ref = SAMPLE_CORPUS[idx]["ref"]
        text = SAMPLE_CORPUS[idx]["text"][:60] + "..."
        
        if score > 0.7:
            print(f"  {i}. âœ“ STRONG MATCH (score: {score:.3f})")
        elif score > 0.6:
            print(f"  {i}. â—‹ GOOD MATCH (score: {score:.3f})")
        elif score > 0.5:
            print(f"  {i}. â—‹ WEAK MATCH (score: {score:.3f})")
        else:
            print(f"  {i}. âœ— NO MATCH (score: {score:.3f})")
        
        print(f"     Ref: {ref}")
        print(f"     Hebrew: {text}")

print("\n" + "="*70)
print("ANALYSIS")
print("="*70)
print("""
With 30 texts instead of 5:
- Scores should be HIGHER (more competition = clearer winners)
- Exact matches should rise to the top
- If scores are still < 0.7, we use HYBRID approach (see below)

HYBRID APPROACH (Best of Both Worlds):
1. Vector search finds top 5 candidates (fast, handles any transliteration)
2. Claude reviews the 5 and picks the best match (accurate, contextual)
3. User gets the right Hebrew term with high confidence

Example:
User: "chezkas rav huna"
Vector: Returns 5 texts mentioning ×—×–×§×” or ×¨×‘ ×”×•× ×
Claude: "The most relevant is Chullin 10a: ×—×–×§×ª ×¨×‘ ×”×•× ×"
Result: Perfect match, infinite scalability, no dictionaries

This is MORE POWERFUL than pure vector search alone!
""")

print("\n" + "="*70)
print("NEXT STEPS")
print("="*70)
print("""
1. Run THIS test (not the old one with 5 texts)
2. Check if scores improve with larger corpus
3. If yes â†’ Proceed with full Sefaria indexing
4. If no â†’ Use hybrid approach (vector + Claude verification)

Either way, you WIN. The vector search handles infinite variations,
Claude handles the final verification. No dictionaries needed.
""")